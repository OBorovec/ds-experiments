{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for experimenting with text processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "/Users/i333584/Projects/ds-experiments/sentiment_analysis_IMDB_reviews/notebooks\n"
    }
   ],
   "source": [
    "# necessary to add root to the path to access core package\n",
    "import sys\n",
    "sys.path.append(sys.path[0] + '/..')\n",
    "# print(sys.path)\n",
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Here add packages you develop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_TEXT = \"\"\"\n",
    "<p style='clear:both;'>Loris gypsum dolour sits is meat, dictum divides sentient per no sole. Requiem effendi and vies, sequel rill animal ea has, ad prod option integer ea. Use it mollies nostrum, per en appropriate negligent. Ala emus no duo, obi verger debus an. Ea elite ague disco bequeath eons. Gracie nominal set id xiv.</p>\n",
    "\n",
    "<p style='clear:both;'>Pro id bonbon accustoms. Stet probates in duo. Set ponder um expedients cu, veil ex quid dictum momentum. Gracie nominal set id xiv. Eel cues linguist efficient ea, veil sale disciple at. Es drams vituperate it, amateur lucid lids ex mesh.</p>\n",
    "\n",
    "<p style='clear:both;'>Brusque croquet pro ea. Deter ornate.</p>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text processing using Regex and BeuatifulSoup\n",
    "\n",
    "* [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "loris gypsum dolour sits is meat dictum divides sentient per no sole requiem effendi and vies sequel rill animal ea has ad prod option integer ea use it mollies nostrum per en appropriate negligent ala emus no duo obi verger debus an ea elite ague disco bequeath eons gracie nominal set id xiv pro id bonbon accustoms stet probates in duo set ponder um expedients cu veil ex quid dictum momentum gracie nominal set id xiv eel cues linguist efficient ea veil sale disciple at es drams vituperate it amateur lucid lids ex mesh brusque croquet pro ea deter ornate \n"
    }
   ],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "REPLACE_NO_SPACE = re.compile(\"[.;:!\\'?,\\\"()\\[\\]]\")\n",
    "REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "\n",
    "def clear_text(text):\n",
    "    soup = BeautifulSoup(text)\n",
    "    text = soup.get_text()\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = REPLACE_NO_SPACE.sub(\"\", text.lower())\n",
    "    text = REPLACE_WITH_SPACE.sub(\" \", text)\n",
    "    return text\n",
    "\n",
    "def split_text(text):\n",
    "    return text.split()\n",
    "\n",
    "print(clear_text(SAMPLE_TEXT))\n",
    "# split_text(clear_text(SAMPLE_TEXT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text processing using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "import nltk\n",
    "# nltk.download() # Uncomment only if you do not the data already downlaoded\n",
    "# Check nltk data by uncommenting following lines\n",
    "from nltk.corpus import brown\n",
    "brown.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['loris', 'gypsum', 'dolour', 'sit', 'meat', 'dictum', 'divide', 'sentient', 'per', 'sole', 'requiem', 'effendi', 'vie', 'sequel', 'rill', 'animal', 'ea', 'ad', 'prod', 'option', 'integer', 'ea', 'use', 'mollies', 'nostrum', 'per', 'en', 'appropriate', 'negligent', 'ala', 'emus', 'duo', 'obi', 'verger', 'debus', 'ea', 'elite', 'ague', 'disco', 'bequeath', 'eons', 'gracie', 'nominal', 'set', 'id', 'xiv', 'pro', 'id', 'bonbon', 'accustom', 'stet', 'probate', 'duo', 'set', 'ponder', 'um', 'expedients', 'cu', 'veil', 'ex', 'quid', 'dictum', 'momentum', 'gracie', 'nominal', 'set', 'id', 'xiv', 'eel', 'cue', 'linguist', 'efficient', 'ea', 'veil', 'sale', 'disciple', 'es', 'drams', 'vituperate', 'amateur', 'lucid', 'lids', 'ex', 'mesh', 'brusque', 'croquet', 'pro', 'ea', 'deter', 'ornate']\n"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def process_text(text):\n",
    "    soup = BeautifulSoup(text)\n",
    "    text = soup.get_text()\n",
    "    words = nltk.word_tokenize(text)\n",
    "    words = [lemmatizer.lemmatize(word, wordnet.VERB) for word in words]\n",
    "    words = [word for word in words if not word in stop_words]\n",
    "    return words\n",
    "\n",
    "print(process_text(clear_text(SAMPLE_TEXT)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Sklearn text feature extraction](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction.text)\n",
    "\n",
    "* [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer)\n",
    "  Convert a collection of text documents to a matrix of token counts\n",
    "\n",
    "```This implementation produces a sparse representation of the counts using scipy.sparse.csr_matrix.```\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['accustoms', 'ad', 'ague', 'ala', 'amateur', 'an', 'and', 'animal', 'appropriate', 'at', 'bequeath', 'bonbon', 'brusque', 'croquet', 'cu', 'cues', 'debus', 'deter', 'dictum', 'disciple', 'disco', 'divides', 'dolour', 'drams', 'duo', 'ea', 'eel', 'effendi', 'efficient', 'elite', 'emus', 'en', 'eons', 'es', 'ex', 'expedients', 'gracie', 'gypsum', 'has', 'id', 'in', 'integer', 'is', 'it', 'lids', 'linguist', 'loris', 'lucid', 'meat', 'mesh', 'mollies', 'momentum', 'negligent', 'no', 'nominal', 'nostrum', 'obi', 'option', 'ornate', 'per', 'ponder', 'pro', 'probates', 'prod', 'quid', 'requiem', 'rill', 'sale', 'sentient', 'sequel', 'set', 'sits', 'sole', 'stet', 'um', 'use', 'veil', 'verger', 'vies', 'vituperate', 'xiv']\n"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform([clear_text(SAMPLE_TEXT)])\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['accustom', 'ad', 'ague', 'ala', 'amateur', 'animal', 'appropriate', 'bequeath', 'bonbon', 'brusque', 'croquet', 'cu', 'cue', 'debus', 'deter', 'dictum', 'disciple', 'disco', 'divide', 'dolour', 'drams', 'duo', 'ea', 'eel', 'effendi', 'efficient', 'elite', 'emus', 'en', 'eons', 'es', 'ex', 'expedients', 'gracie', 'gypsum', 'id', 'integer', 'lids', 'linguist', 'loris', 'lucid', 'meat', 'mesh', 'mollies', 'momentum', 'negligent', 'nominal', 'nostrum', 'obi', 'option', 'ornate', 'per', 'ponder', 'pro', 'probate', 'prod', 'quid', 'requiem', 'rill', 'sale', 'sentient', 'sequel', 'set', 'sit', 'sole', 'stet', 'um', 'use', 'veil', 'verger', 'vie', 'vituperate', 'xiv']\n"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform([' '.join(process_text(clear_text(SAMPLE_TEXT)))])\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following text clearning function was moved to lib folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "\n",
    "REPLACE_NO_SPACE = re.compile(\"[.;:!\\'?,\\\"()\\[\\]]\")\n",
    "REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "LEMMATIZER = WordNetLemmatizer()\n",
    "STEMMER = PorterStemmer()\n",
    "ENGLISH_STOP_WORDS = set(stopwords.words(\"english\"))\n",
    "\n",
    "def clean_text_en(text):\n",
    "    soup = BeautifulSoup(text)\n",
    "    text = soup.get_text()\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = REPLACE_NO_SPACE.sub(\"\", text.lower())\n",
    "    text = REPLACE_WITH_SPACE.sub(\" \", text)\n",
    "    words = nltk.word_tokenize(text)\n",
    "    words = [lemmatizer.lemmatize(word, wordnet.VERB) for word in words]\n",
    "    words = [word for word in words if not word in stop_words]\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'loris gypsum dolour sit meat dictum divide sentient per sole requiem effendi vie sequel rill animal ea ad prod option integer ea use mollies nostrum per en appropriate negligent ala emus duo obi verger debus ea elite ague disco bequeath eons gracie nominal set id xiv pro id bonbon accustom stet probate duo set ponder um expedients cu veil ex quid dictum momentum gracie nominal set id xiv eel cue linguist efficient ea veil sale disciple es drams vituperate amateur lucid lids ex mesh brusque croquet pro ea deter ornate'"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "clean_text_en(SAMPLE_TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38164bit5f95d4508e474177a9f07d23ac671d58",
   "display_name": "Python 3.8.1 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}